## Model-Market – Evaluations (A)

| **Name**                  | **Known For**                                                  | **Link** |
|----------------------------|----------------------------------------------------------------|----------|
| AutoEval                   | Automated evaluation toolkit for ML/DL models.                | [GitHub](https://github.com/microsoft/autoeval) |
| AI Explainability 360      | Toolkit by IBM for evaluating fairness and explainability.     | [GitHub](https://github.com/IBM/AIX360) |
| Arctic Evaluator           | Model evaluation pipeline for reproducible research.           | [GitHub](https://github.com/yourusername/arctic-evaluator) |
| Awesome Evaluation Tools   | Curated list of ML model evaluation frameworks.                | [GitHub](https://github.com/ml-tooling/awesome-evaluation-tools) |

## Model-Market – Evaluations (B)

| **Name**                  | **Known For**                                                  | **Link** |
|----------------------------|----------------------------------------------------------------|----------|
| BenchmarkML                | Framework for benchmarking ML models across tasks.             | [GitHub](https://github.com/benchmarkml/benchmarkml) |
| BigBench Evaluator          | Evaluation suite for BIG-bench NLP tasks.                     | [GitHub](https://github.com/allenai/BBH) |
| BlueEval                   | Evaluation framework for text-to-speech and audio models.      | [GitHub](https://github.com/blue-eval/blue-eval) |
| BLiMP Evaluations           | Benchmarking English linguistic minimal pairs in NLP models.  | [GitHub](https://github.com/alexwarstadt/blimp) |

## Model-Market – Evaluations (C)

| **Name**                  | **Known For**                                                    | **Link** |
|----------------------------|------------------------------------------------------------------|----------|
| CMU-MoCap Eval             | Evaluation dataset and metrics for motion capture models.       | [GitHub](https://github.com/CMU-Perceptual-Computing-Lab/CMU-MoCap) |
| CoNLL Shared Task Eval      | Standard evaluation for NLP tasks like NER, POS tagging.        | [Website](https://www.conll.org/) |
| ConvEval                   | Evaluation framework for conversational AI and dialogue models.| [GitHub](https://github.com/conv-eval/conv-eval) |
| CrowdAI Benchmark          | Platform for evaluating computer vision and ML competitions.    | [Website](https://www.crowdai.org/) |

## Model-Market – Evaluations (D)

| **Name**                  | **Known For**                                                    | **Link** |
|----------------------------|------------------------------------------------------------------|----------|
| DAWNBench                  | Benchmark for end-to-end training and inference performance of ML models. | [GitHub](https://github.com/DAWNBench/DAWNBench) |
| DeepEval                   | Evaluation framework for deep learning models across tasks.     | [GitHub](https://github.com/deep-eval/deep-eval) |
| DLBENCH                     | Benchmark for deep learning models on multiple datasets.       | [GitHub](https://github.com/dlbench/dlbench) |
| DNN Benchmark               | Evaluation of deep neural networks for speed and accuracy.     | [GitHub](https://github.com/mihaimaruseac/DeepLearningBenchmark) |

