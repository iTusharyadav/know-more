## Model-Market – Evaluations (A)

| **Name**                  | **Known For**                                                  | **Link** |
|----------------------------|----------------------------------------------------------------|----------|
| AutoEval                   | Automated evaluation toolkit for ML/DL models.                | [GitHub](https://github.com/microsoft/autoeval) |
| AI Explainability 360      | Toolkit by IBM for evaluating fairness and explainability.     | [GitHub](https://github.com/IBM/AIX360) |
| Arctic Evaluator           | Model evaluation pipeline for reproducible research.           | [GitHub](https://github.com/yourusername/arctic-evaluator) |
| Awesome Evaluation Tools   | Curated list of ML model evaluation frameworks.                | [GitHub](https://github.com/ml-tooling/awesome-evaluation-tools) |

## Model-Market – Evaluations (B)

| **Name**                  | **Known For**                                                  | **Link** |
|----------------------------|----------------------------------------------------------------|----------|
| BenchmarkML                | Framework for benchmarking ML models across tasks.             | [GitHub](https://github.com/benchmarkml/benchmarkml) |
| BigBench Evaluator          | Evaluation suite for BIG-bench NLP tasks.                     | [GitHub](https://github.com/allenai/BBH) |
| BlueEval                   | Evaluation framework for text-to-speech and audio models.      | [GitHub](https://github.com/blue-eval/blue-eval) |
| BLiMP Evaluations           | Benchmarking English linguistic minimal pairs in NLP models.  | [GitHub](https://github.com/alexwarstadt/blimp) |

## Model-Market – Evaluations (C)

| **Name**                  | **Known For**                                                    | **Link** |
|----------------------------|------------------------------------------------------------------|----------|
| CMU-MoCap Eval             | Evaluation dataset and metrics for motion capture models.       | [GitHub](https://github.com/CMU-Perceptual-Computing-Lab/CMU-MoCap) |
| CoNLL Shared Task Eval      | Standard evaluation for NLP tasks like NER, POS tagging.        | [Website](https://www.conll.org/) |
| ConvEval                   | Evaluation framework for conversational AI and dialogue models.| [GitHub](https://github.com/conv-eval/conv-eval) |
| CrowdAI Benchmark          | Platform for evaluating computer vision and ML competitions.    | [Website](https://www.crowdai.org/) |

## Model-Market – Evaluations (D)

| **Name**                  | **Known For**                                                    | **Link** |
|----------------------------|------------------------------------------------------------------|----------|
| DAWNBench                  | Benchmark for end-to-end training and inference performance of ML models. | [GitHub](https://github.com/DAWNBench/DAWNBench) |
| DeepEval                   | Evaluation framework for deep learning models across tasks.     | [GitHub](https://github.com/deep-eval/deep-eval) |
| DLBENCH                     | Benchmark for deep learning models on multiple datasets.       | [GitHub](https://github.com/dlbench/dlbench) |
| DNN Benchmark               | Evaluation of deep neural networks for speed and accuracy.     | [GitHub](https://github.com/mihaimaruseac/DeepLearningBenchmark) |

## Model-Market – Evaluations (E)

| **Name**                  | **Known For**                                                                 | **Link** |
|----------------------------|-------------------------------------------------------------------------------|----------|
| EBench                     | Evaluation framework for energy-efficient ML model benchmarking.              | [GitHub](https://github.com/ebench/ebench) |
| EvalAI                     | Platform for evaluating and comparing AI models across challenges.            | [GitHub](https://github.com/Cloud-CV/EvalAI) |
| EvalRS                     | Evaluation framework for recommender systems.                                 | [GitHub](https://github.com/reczoo/EvalRS-CIKM-2022) |
| EvalPlus                   | Benchmark for evaluating code generation models.                             | [GitHub](https://github.com/evalplus/evalplus) |
| EvalScope                  | LLM evaluation and benchmarking toolkit.                                     | [GitHub](https://github.com/aliemteam/evalscope) |
| EvalFarm                   | Centralized evaluation platform for ML and NLP models.                       | [GitHub](https://github.com/evalfarm/evalfarm) |
| EMEval                     | Framework for model evaluation across multimodal tasks.                      | [GitHub](https://github.com/emeval/emeval) |
| EasyEval                   | Lightweight toolkit for evaluating deep learning and ML pipelines.            | [GitHub](https://github.com/easyeval/easyeval) |

## Model-Market – Evaluations (F)

| **Name**                  | **Known For**                                                                 | **Link** |
|----------------------------|-------------------------------------------------------------------------------|----------|
| FairEval                   | Evaluation framework for fairness and bias in ML models.                     | [GitHub](https://github.com/IBM/fairness-evaluation) |
| FastEval                   | Lightweight toolkit for fast evaluation of machine learning models.          | [GitHub](https://github.com/fast-eval/fast-eval) |
| FineEval                   | Fine-grained evaluation metrics for NLP and vision models.                   | [GitHub](https://github.com/fineeval/fineeval) |
| FlexEval                   | Modular evaluation suite supporting custom metrics and tasks.                 | [GitHub](https://github.com/flexeval/flexeval) |
| FoundryEval                | Model evaluation hub by Hugging Face for community benchmarks.               | [Website](https://huggingface.co/evaluate) |
| FLEET                      | Evaluation framework for federated learning models.                          | [GitHub](https://github.com/epfl-lts2/fleet) |
| FMEval                     | Amazon’s open-source toolkit for foundation model evaluation.                | [GitHub](https://github.com/awslabs/fmeval) |
| FullEval                   | End-to-end evaluation workflow for ML lifecycle management.                   | [GitHub](https://github.com/fulleval/fulleval) |

## Model-Market – Evaluations (G)

| **Name**                  | **Known For**                                                                 | **Link** |
|----------------------------|-------------------------------------------------------------------------------|----------|
| GLUE Benchmark             | Standard evaluation benchmark for NLP models.                                 | [Website](https://gluebenchmark.com/) |
| GloVeEval                  | Evaluation tools for word embeddings and vector representations.              | [GitHub](https://github.com/stanfordnlp/GloVe) |
| GaugeEval                  | Evaluation framework for generative AI model outputs.                         | [GitHub](https://github.com/gauge-eval/gauge) |
| GeoEval                    | Geographic and spatial ML model evaluation toolkit.                           | [GitHub](https://github.com/GeoEval/GeoEval) |
| GPT-Eval                   | Automated evaluation scripts for GPT-style language models.                   | [GitHub](https://github.com/EleutherAI/gpt-eval) |
| GradingML                  | ML evaluation toolkit for grading and scoring model predictions.              | [GitHub](https://github.com/gradingml/gradingml) |
| GreenML                     | Metrics and evaluation for energy-efficient ML models.                        | [GitHub](https://github.com/GreenML/GreenML) |
| GCP-Eval                   | Google Cloud Platform evaluation tools for ML pipelines.                      | [Website](https://cloud.google.com/ai-platform) |

## Model-Market – Evaluations (H)

| **Name**                  | **Known For**                                                                 | **Link** |
|----------------------------|-------------------------------------------------------------------------------|----------|
| HANS Benchmark             | Heuristic Analysis for NLI Systems, evaluating model robustness in NLP.       | [GitHub](https://github.com/facebookresearch/NLI-HANS) |
| HAR-Eval                   | Evaluation toolkit for Human Activity Recognition models.                     | [GitHub](https://github.com/har-eval/har-eval) |
| HealthML Benchmark         | Evaluation dataset and metrics for medical and health ML models.              | [Website](https://healthml.org/) |
| HuggingFace Eval           | Evaluation suite for various NLP and multimodal models hosted on HuggingFace. | [Website](https://huggingface.co/metrics) |
| HypeBench                  | Benchmarking platform for large-scale language model performance evaluation.  | [GitHub](https://github.com/hypebench/hypebench) |

## Model-Market – Evaluations (I)

| **Name**                  | **Known For**                                                                 | **Link** |
|----------------------------|-------------------------------------------------------------------------------|----------|
| ImageNet Benchmark         | Standard evaluation dataset for image classification and vision models.       | [Website](http://www.image-net.org/) |
| IWSLT Evaluation           | Evaluation datasets for machine translation models in low-resource languages.| [Website](https://iwslt.org/) |
| IMDB Sentiment Eval        | Benchmark for evaluating sentiment analysis models on IMDB reviews.          | [GitHub](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) |
| Inference Time Benchmark   | Evaluation suite to measure model inference speed across hardware.            | [GitHub](https://github.com/pytorch/benchmark) |
| IQA Benchmark              | Image quality assessment benchmark for evaluating generative and enhancement models. | [GitHub](https://github.com/chaofengc/IQA-PyTorch) |

## Model-Market – Evaluations (J)

| **Name**                     | **Known For**                                                                 | **Link** |
|-------------------------------|-------------------------------------------------------------------------------|----------|
| JGLUE Benchmark               | Japanese General Language Understanding Evaluation benchmark for NLP models. | [GitHub](https://github.com/yahoojapan/JGLUE) |
| JHMDB Dataset Evaluation      | Evaluation benchmark for human motion and action recognition models.          | [Website](http://jhmdb.is.tue.mpg.de/) |
| JSonBench                    | JSON performance benchmark for testing model output serialization efficiency. | [GitHub](https://github.com/fabienrenaud/java-json-benchmark) |
| JAXBench                     | Performance and accuracy benchmarking suite for JAX-based ML models.          | [GitHub](https://github.com/google/jax-benchmarks) |
| Joint Embedding Benchmark     | Evaluates multimodal (text-image) representation alignment models.            | [GitHub](https://github.com/mlfoundations/open_clip) |

## Model-Market – Evaluations (K)

| **Name**                     | **Known For**                                                                 | **Link** |
|-------------------------------|-------------------------------------------------------------------------------|----------|
| KITTI Benchmark               | Autonomous driving dataset evaluation for object detection, tracking, and segmentation. | [Website](http://www.cvlibs.net/datasets/kitti/) |
| Kinetics Benchmark            | Large-scale video dataset benchmark for action recognition models.            | [Website](https://deepmind.com/research/open-source/kinetics) |
| Kaggle Model Evaluation       | Platform-provided evaluation metrics and leaderboards for diverse ML competitions. | [Website](https://www.kaggle.com/) |
| Knowledge Graph Completion Benchmark | Evaluation datasets for link prediction and knowledge graph completion models. | [GitHub](https://github.com/TimDettmers/ConvE) |
| Keypoint Detection Benchmark  | Evaluates human pose estimation and keypoint detection models.               | [GitHub](https://github.com/princeton-vl/pose-hg-train) |

## Model-Market – Evaluations (L)

| **Name**                     | **Known For**                                                                 | **Link** |
|-------------------------------|-------------------------------------------------------------------------------|----------|
| LFW (Labeled Faces in the Wild) | Benchmark for face recognition models, measuring accuracy on unconstrained face images. | [Website](http://vis-www.cs.umass.edu/lfw/) |
| LibriSpeech Evaluation        | Evaluation dataset for speech recognition models.                             | [Website](https://www.openslr.org/12/) |
| LAMBADA Benchmark             | Evaluates language models on reading comprehension and long-range context understanding. | [GitHub](https://github.com/cybertronai/lamdada) |
| LISA Traffic Sign Benchmark   | Evaluates traffic sign detection and classification models.                   | [Website](http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html) |
| LAMB Evaluation Suite         | Metrics and datasets for evaluating large language models in NLP tasks.       | [GitHub](https://github.com/EleutherAI/lamb) |

## Model-Market – Evaluations (M)

| **Name**                     | **Known For**                                                                 | **Link** |
|-------------------------------|-------------------------------------------------------------------------------|----------|
| MIMIC-III Benchmark           | Evaluates models on medical data prediction, including ICU patient outcomes. | [Website](https://mimic.physionet.org/) |
| MNIST Evaluation              | Benchmark for evaluating image classification models on handwritten digits.  | [Website](http://yann.lecun.com/exdb/mnist/) |
| MS COCO Evaluation            | Evaluates object detection, segmentation, and captioning models.              | [Website](https://cocodataset.org/#home) |
| MuJoCo Benchmark              | Evaluates reinforcement learning models on continuous control tasks.          | [Website](https://www.roboti.us/index.html) |
| MultiNLI Evaluation           | Benchmark for evaluating natural language inference models.                   | [Website](https://www.nyu.edu/projects/bowman/multinli/) |

## Model-Market – Evaluations (N)

| **Name**                     | **Known For**                                                                 | **Link** |
|-------------------------------|-------------------------------------------------------------------------------|----------|
| NER Benchmark (CoNLL-2003)    | Evaluates named entity recognition models across multiple languages.         | [Website](https://www.clips.uantwerpen.be/conll2003/ner/) |
| NLP GLUE Benchmark            | Evaluates general language understanding across various NLP tasks.           | [Website](https://gluebenchmark.com/) |
| NIST Machine Translation Eval | Evaluates machine translation quality using BLEU and other metrics.          | [Website](https://www.nist.gov/itl/iad/mig/openmt) |
| NSynth Evaluation             | Tests generative models on audio synthesis and representation learning.      | [Website](https://magenta.tensorflow.org/nsynth) |
| NuScenes Evaluation           | Benchmark for evaluating autonomous driving perception models.               | [Website](https://www.nuscenes.org/) |

## Model-Market – Evaluations (O)

| **Name**                     | **Known For**                                                                 | **Link** |
|-------------------------------|-------------------------------------------------------------------------------|----------|
| ObjectNet Benchmark           | Evaluates object recognition models under real-world distribution shifts.     | [Website](https://objectnet.dev/) |
| OCID Dataset Evaluation       | Used for benchmarking 3D object segmentation and detection in cluttered scenes.| [GitHub](https://github.com/kloping/OCID) |
| OCR Benchmark (OpenImages)    | Evaluates OCR model performance on natural scenes and diverse fonts.          | [Website](https://storage.googleapis.com/openimages/web/index.html) |
| OpenAI Evaluations            | Framework for assessing AI model capabilities across tasks and domains.       | [GitHub](https://github.com/openai/evals) |
| OpenML Benchmark Suites       | Standardized tasks and datasets for ML model evaluation.                     | [Website](https://www.openml.org/) |
| OpenVINO Model Benchmark Tool | Evaluates model performance on Intel hardware accelerators.                   | [Website](https://docs.openvino.ai/latest/omz_tools_benchmark_readme.html) |
| ORB-SLAM Evaluation Dataset   | Used for evaluating visual SLAM algorithms’ accuracy and robustness.          | [GitHub](https://github.com/raulmur/ORB_SLAM2) |
| OSBench (Open Source Benchmark)| General benchmark suite for open ML/DL models’ inference and accuracy testing.| [GitHub](https://github.com/OSBench) |
| OTB Benchmark (Object Tracking Benchmark) | Evaluates single-object tracking algorithms with standardized metrics. | [Website](http://cvlab.hanyang.ac.kr/tracker_benchmark/) |
| Oxford-IIIT Pet Benchmark     | Evaluates image segmentation and classification models.                      | [Website](https://www.robots.ox.ac.uk/~vgg/data/pets/) |

## Model-Market – Evaluations (P)

| **Name**                     | **Known For**                                                                 | **Link** |
|-------------------------------|-------------------------------------------------------------------------------|----------|
| Pascal VOC Benchmark           | Standard benchmark for object detection, segmentation, and classification.    | [Website](http://host.robots.ox.ac.uk/pascal/VOC/) |
| Penn Treebank Evaluation       | Evaluates NLP models on language modeling tasks.                               | [Website](https://catalog.ldc.upenn.edu/LDC99T42) |
| Physionet Challenge            | Evaluates models on medical signal processing and prediction tasks.           | [Website](https://physionet.org/challenge/) |
| Platypus Benchmark             | Benchmark for multi-object tracking in video sequences.                        | [GitHub](https://github.com/Platypus-MOT/platypus) |
| PASC Benchmark                 | Performance Assessment of Speech and Audio Classification models.             | [Website](https://www.cs.tut.fi/sgn/arg/pascal-benchmark/) |
| PoseTrack Benchmark            | Evaluates human pose estimation and tracking models.                           | [Website](https://posetrack.net/) |
| PTB XL Benchmark               | ECG classification benchmark dataset for evaluating deep learning models.     | [Website](https://www.physionet.org/content/ptb-xl/1.0.1/) |
| PyTorch ImageNet Benchmark     | Evaluates image classification model performance on ImageNet dataset.         | [GitHub](https://github.com/pytorch/vision/tree/main/benchmarks) |
| PyTorch NLP Evaluation Suite   | Suite to benchmark NLP models for tasks like translation, QA, and sentiment.   | [GitHub](https://github.com/pytorch/text) |
| PyTorch Video Benchmark        | Evaluates video understanding models for action recognition tasks.            | [GitHub](https://github.com/pytorch/vision/tree/main/references/video_classification) |

## Model-Market – Evaluations (Q)

| **Name**                     | **Known For**                                                                 | **Link** |
|-------------------------------|-------------------------------------------------------------------------------|----------|
| Queen’s Speech Recognition Benchmark | Evaluates speech recognition models on Queen’s English accents.             | [Website](https://www.qsrbenchmark.org/) |
| QuickDraw Evaluation Dataset  | Evaluates sketch recognition and drawing models using Google’s QuickDraw data. | [Website](https://quickdraw.withgoogle.com/data) |
| Quora Question Pairs Benchmark | Evaluates NLP models for semantic similarity and duplicate question detection. | [Website](https://www.kaggle.com/c/quora-question-pairs) |
| QED Benchmark                 | Evaluates question answering and entity detection models for English language. | [Website](https://github.com/allenai/qed) |
| QASC Benchmark                | Evaluates models on multi-choice science questions requiring reasoning.        | [Website](https://leaderboard.allenai.org/qasc/submissions/get-started) |
| QNLI Benchmark                | Question Natural Language Inference evaluation dataset for NLP tasks.         | [Website](https://huggingface.co/datasets/glue/viewer/qnli) |
| QUAC Benchmark                | Question Answering in Context evaluation for conversational QA models.        | [Website](https://quac.ai/) |
| QuranQA Benchmark             | Evaluates question answering systems on the Quranic text.                     | [Website](https://quranqa.org/) |

## Model-Market – Evaluations (R)

| **Name**                          | **Known For**                                                                   | **Link** |
|-----------------------------------|----------------------------------------------------------------------------------|----------|
| RACE Benchmark                    | Reading comprehension evaluation for middle and high school English exams.       | [Website](http://www.cs.cmu.edu/~glai1/data/race/) |
| RealToxicityPrompts               | Evaluates toxicity in model-generated text prompts.                             | [GitHub](https://github.com/allenai/real-toxicity-prompts) |
| ReClor Benchmark                  | Logical reasoning benchmark for reading comprehension tasks.                     | [Website](https://eval.ai/web/challenges/challenge-page/503/overview) |
| RefCOCO Dataset                   | Evaluates referring expression comprehension and grounding in images.            | [Website](https://github.com/lichengunc/refer) |
| RetrievalQA Benchmark             | Evaluates models on retrieval-based question answering tasks.                    | [Website](https://huggingface.co/datasets/retrieval-qa) |
| RobustQA Benchmark                | Tests the robustness of QA models against adversarial examples.                  | [GitHub](https://github.com/marcotcr/robustqa) |
| RocStories (Story Cloze Test)     | Evaluates commonsense reasoning in story completion tasks.                       | [Website](https://cs.rochester.edu/nlp/rocstories/) |
| Rotten Tomatoes Sentiment Dataset | Evaluates sentiment classification models on movie reviews.                      | [Website](https://huggingface.co/datasets/rotten_tomatoes) |
| RTE (Recognizing Textual Entailment) | Evaluates natural language inference capabilities.                            | [Website](https://huggingface.co/datasets/glue/viewer/rte) |
| RULER Benchmark                   | Evaluates reasoning and utility of LLMs through logical tasks.                   | [GitHub](https://github.com/allenai/ruler) |
| RAGAS Benchmark                   | Evaluates retrieval-augmented generation systems.                               | [GitHub](https://github.com/explodinggradients/ragas) |
| RuSentiment Benchmark             | Evaluates sentiment analysis for Russian-language datasets.                      | [GitHub](https://github.com/text-machine-lab/rusentiment) |
| RWKV Evaluation Suite             | Benchmarks RWKV language models for efficiency and accuracy.                     | [GitHub](https://github.com/BlinkDL/RWKV-LM) |
| ReasonBench                      | Evaluates models on complex reasoning and step-by-step problem solving.          | [Website](https://reasonbench.ai/) |
| RedTeamEval                      | Evaluation framework for testing model vulnerability to harmful outputs.         | [GitHub](https://github.com/Anthropic/red-team-eval) |

## Model-Market – Evaluations (S)

| **Name**                          | **Known For**                                                                   | **Link** |
|-----------------------------------|----------------------------------------------------------------------------------|----------|
| SQuAD (Stanford Question Answering Dataset) | Benchmark for reading comprehension and QA tasks.                   | [Website](https://rajpurkar.github.io/SQuAD-explorer/) |
| SuperGLUE Benchmark               | Advanced general language understanding benchmark.                              | [Website](https://super.gluebenchmark.com/) |
| SentEval                         | Evaluates quality of sentence embeddings and semantic representations.           | [GitHub](https://github.com/facebookresearch/SentEval) |
| SciEval                          | Evaluation framework for scientific text summarization and reasoning.            | [Website](https://huggingface.co/datasets/scieval) |
| SQuAD2.0                         | QA benchmark including unanswerable questions.                                  | [Website](https://rajpurkar.github.io/SQuAD-explorer/) |
| SST-2 (Stanford Sentiment Treebank) | Sentiment classification benchmark for text models.                            | [Website](https://huggingface.co/datasets/sst2) |
| SHAPE Benchmark                  | Evaluates spatial reasoning in visual question answering.                        | [GitHub](https://github.com/facebookresearch/shape-benchmark) |
| SkillBench                       | Benchmarks skill-based LLM reasoning and multi-step task-solving.                | [Website](https://huggingface.co/datasets/skillbench) |
| StoryCloze Test                  | Commonsense reasoning evaluation for story completion.                           | [Website](https://cs.rochester.edu/nlp/rocstories/) |
| SummEval                         | Evaluation benchmark for summarization model quality.                            | [GitHub](https://github.com/Yale-LILY/SummEval) |
| SuperGLUE Diagnostic             | Subset for diagnostic evaluation of GLUE tasks.                                 | [Website](https://super.gluebenchmark.com/tasks) |
| ScienceQA                        | Benchmark for multimodal scientific question answering.                          | [GitHub](https://github.com/lupantech/ScienceQA) |
| Stanford Sentiment Treebank (SST) | Fine-grained sentiment classification benchmark.                                | [Website](https://nlp.stanford.edu/sentiment/index.html) |
| Scifact                          | Evaluates scientific fact verification and claim validation.                     | [Website](https://github.com/allenai/scifact) |
| SmartEval                        | Benchmark for evaluating LLMs on smart assistant capabilities.                   | [Website](https://smarteval.ai/) |
| SocialIQa                        | Commonsense social interaction reasoning benchmark.                              | [Website](https://allenai.org/data/socialiqa) |
| SafeBench                        | Evaluates model safety and robustness against adversarial prompts.               | [GitHub](https://github.com/thu-safe-ai/safebench) |
| SymEval                          | Symbolic reasoning evaluation for math and logic models.                         | [Website](https://symbench.ai/) |
| STS-B (Semantic Textual Similarity Benchmark) | Evaluates semantic similarity between sentence pairs.             | [Website](https://huggingface.co/datasets/stsb_multi_mt) |
| SpeechBench                      | Evaluation dataset for automatic speech recognition (ASR) models.                | [Website](https://speechbench.ai/) |

## Model-Market – Evaluations (T)

| **Name**                           | **Known For**                                                                  | **Link** |
|------------------------------------|--------------------------------------------------------------------------------|----------|
| TruthfulQA                        | Evaluates truthfulness and factual accuracy of language models.                | [GitHub](https://github.com/sylinrl/TruthfulQA) |
| TydiQA                             | Multilingual question answering benchmark.                                    | [Website](https://github.com/google-research-datasets/tydiqa) |
| TREC                               | Benchmark for text classification and information retrieval.                  | [Website](https://trec.nist.gov/) |
| TaskBench                          | Large-scale benchmark suite for diverse NLP task evaluation.                  | [GitHub](https://github.com/TaskBench/TaskBench) |
| TAPAS Evaluation Suite             | Evaluation framework for table question answering.                            | [Website](https://github.com/google-research/tapas) |
| ToxBench                           | Benchmarks for toxicity and bias detection in language models.                | [Website](https://huggingface.co/datasets/toxigen) |
| TMLR Eval                          | Evaluation repository from *Transactions on Machine Learning Research*.       | [Website](https://tmlr.org/evaluation/) |
| TextBench                         | Benchmark for general text generation tasks.                                 | [GitHub](https://github.com/THUNLP-MT/TextBench) |
| TabFact                           | Fact-checking benchmark for tabular data.                                     | [GitHub](https://github.com/wenhuchen/Table-Fact-Checking) |
| TinyBench                         | Lightweight benchmark for small language models.                              | [Website](https://huggingface.co/datasets/tinybench) |
| The Pile Benchmark                | Evaluation suite using large-scale open text corpus.                          | [Website](https://pile.eleuther.ai/) |
| TruthArena                        | Evaluates LLM factual consistency and truth alignment.                        | [Website](https://huggingface.co/datasets/trutharena) |
| TorchBench                        | Benchmark for PyTorch-based deep learning models.                             | [GitHub](https://github.com/pytorch/benchmark) |
| TextAttack Eval                   | Evaluation framework for adversarial robustness of NLP models.                | [GitHub](https://github.com/QData/TextAttack) |
| TensorFlow Model Garden Benchmark | Standardized evaluation of TensorFlow models.                                 | [GitHub](https://github.com/tensorflow/models) |
| TMLU (Test of Machine Language Understanding) | General benchmark for LLM comprehension and reasoning.            | [Website](https://tmlu.ai/) |
| TuringBench                       | Benchmark for machine text detection and AI-written text evaluation.          | [GitHub](https://github.com/TuringBench/TuringBench) |
| TimeEval                          | Benchmarking framework for time series anomaly detection models.              | [GitHub](https://github.com/TimeEval/TimeEval) |
| TinyStories Eval                  | Benchmark for evaluating small-scale story generation models.                 | [GitHub](https://github.com/roneneldan/TinyStories) |
| TabularBench                      | Evaluates model performance on structured/tabular data tasks.                 | [GitHub](https://github.com/automl/TabularBench) |

## Model-Market – Evaluations (U)

| **Name**                           | **Known For**                                                                  | **Link** |
|------------------------------------|--------------------------------------------------------------------------------|----------|
| UnifyBenchmark                    | Unified evaluation framework for comparing LLMs across diverse tasks.          | [GitHub](https://github.com/UnifyAI/unify-benchmark) |
| UL2Eval                           | Evaluation suite for Unifying Language Learning models (Google Research).      | [Website](https://github.com/google-research/google-research/tree/master/ul2) |
| Unbabel COMET                     | Framework for evaluating machine translation quality.                          | [GitHub](https://github.com/Unbabel/COMET) |
| Unsupervised Evaluation Toolkit   | Toolkit for zero-shot evaluation of NLP and CV models.                         | [GitHub](https://github.com/unsup-eval/toolkit) |
| ULMFiT Benchmark                  | Benchmark for fine-tuning and evaluating pretrained LMs on various tasks.      | [Website](https://fastai.github.io/) |
| UnifiedQA                         | Multi-task question answering benchmark across QA datasets.                    | [Website](https://allenai.org/data/unifiedqa) |
| Unsupervised SentEval             | Framework for evaluating sentence embeddings.                                  | [GitHub](https://github.com/facebookresearch/SentEval) |
| UniversalBench                    | Evaluation suite for multilingual and multi-domain models.                     | [GitHub](https://github.com/UniversalBench/UniversalBench) |
| UMAPEval                          | Benchmark for evaluating dimensionality reduction on embeddings.               | [Website](https://umap-learn.readthedocs.io/en/latest/) |
| UniEval                           | Unified evaluation for text generation and summarization.                      | [GitHub](https://github.com/maszhongming/UniEval) |
| Uber Ludwig Benchmarks            | Model comparison and evaluation with Ludwig AutoML framework.                  | [GitHub](https://github.com/ludwig-ai/ludwig-benchmarks) |
| UncertaintyBenchmark              | Evaluating model calibration and uncertainty estimation.                       | [GitHub](https://github.com/uncertainty-benchmark/uncertainty-benchmark) |
| UniterEval                        | Benchmarking vision-language pretraining models.                               | [GitHub](https://github.com/ChenRocks/UNITER) |
| UCR Time Series Archive           | Evaluation benchmark for time series classification.                           | [Website](https://www.cs.ucr.edu/~eamonn/time_series_data_2018/) |
| Unbabel Eval                      | Machine translation human evaluation metrics and datasets.                     | [Website](https://unbabel.com/research) |
| UDTBench                          | Benchmark for unsupervised domain translation.                                 | [GitHub](https://github.com/udtbench/udtbench) |
| ULTRA                             | Unified benchmark for ranking and retrieval tasks.                             | [GitHub](https://github.com/ULTRA-Benchmark/ULTRA) |
| UXEval                            | Benchmark for evaluating AI-generated UX design recommendations.               | [Website](https://uxeval.org) |
| UrbanBench                        | Benchmark for evaluating ML models on urban data and smart city applications.  | [GitHub](https://github.com/UrbanBench/UrbanBench) |
| Unified Model Evaluation Hub      | Centralized repository for ML/DL model comparisons and metrics.                | [Website](https://huggingface.co/spaces/evaluation-hub) |

