## Model-Market – Evaluations (A)

| **Name**                  | **Known For**                                                  | **Link** |
|----------------------------|----------------------------------------------------------------|----------|
| AutoEval                   | Automated evaluation toolkit for ML/DL models.                | [GitHub](https://github.com/microsoft/autoeval) |
| AI Explainability 360      | Toolkit by IBM for evaluating fairness and explainability.     | [GitHub](https://github.com/IBM/AIX360) |
| Arctic Evaluator           | Model evaluation pipeline for reproducible research.           | [GitHub](https://github.com/yourusername/arctic-evaluator) |
| Awesome Evaluation Tools   | Curated list of ML model evaluation frameworks.                | [GitHub](https://github.com/ml-tooling/awesome-evaluation-tools) |

## Model-Market – Evaluations (B)

| **Name**                  | **Known For**                                                  | **Link** |
|----------------------------|----------------------------------------------------------------|----------|
| BenchmarkML                | Framework for benchmarking ML models across tasks.             | [GitHub](https://github.com/benchmarkml/benchmarkml) |
| BigBench Evaluator          | Evaluation suite for BIG-bench NLP tasks.                     | [GitHub](https://github.com/allenai/BBH) |
| BlueEval                   | Evaluation framework for text-to-speech and audio models.      | [GitHub](https://github.com/blue-eval/blue-eval) |
| BLiMP Evaluations           | Benchmarking English linguistic minimal pairs in NLP models.  | [GitHub](https://github.com/alexwarstadt/blimp) |

## Model-Market – Evaluations (C)

| **Name**                  | **Known For**                                                    | **Link** |
|----------------------------|------------------------------------------------------------------|----------|
| CMU-MoCap Eval             | Evaluation dataset and metrics for motion capture models.       | [GitHub](https://github.com/CMU-Perceptual-Computing-Lab/CMU-MoCap) |
| CoNLL Shared Task Eval      | Standard evaluation for NLP tasks like NER, POS tagging.        | [Website](https://www.conll.org/) |
| ConvEval                   | Evaluation framework for conversational AI and dialogue models.| [GitHub](https://github.com/conv-eval/conv-eval) |
| CrowdAI Benchmark          | Platform for evaluating computer vision and ML competitions.    | [Website](https://www.crowdai.org/) |

## Model-Market – Evaluations (D)

| **Name**                  | **Known For**                                                    | **Link** |
|----------------------------|------------------------------------------------------------------|----------|
| DAWNBench                  | Benchmark for end-to-end training and inference performance of ML models. | [GitHub](https://github.com/DAWNBench/DAWNBench) |
| DeepEval                   | Evaluation framework for deep learning models across tasks.     | [GitHub](https://github.com/deep-eval/deep-eval) |
| DLBENCH                     | Benchmark for deep learning models on multiple datasets.       | [GitHub](https://github.com/dlbench/dlbench) |
| DNN Benchmark               | Evaluation of deep neural networks for speed and accuracy.     | [GitHub](https://github.com/mihaimaruseac/DeepLearningBenchmark) |

## Model-Market – Evaluations (E)

| **Name**                  | **Known For**                                                                 | **Link** |
|----------------------------|-------------------------------------------------------------------------------|----------|
| EBench                     | Evaluation framework for energy-efficient ML model benchmarking.              | [GitHub](https://github.com/ebench/ebench) |
| EvalAI                     | Platform for evaluating and comparing AI models across challenges.            | [GitHub](https://github.com/Cloud-CV/EvalAI) |
| EvalRS                     | Evaluation framework for recommender systems.                                 | [GitHub](https://github.com/reczoo/EvalRS-CIKM-2022) |
| EvalPlus                   | Benchmark for evaluating code generation models.                             | [GitHub](https://github.com/evalplus/evalplus) |
| EvalScope                  | LLM evaluation and benchmarking toolkit.                                     | [GitHub](https://github.com/aliemteam/evalscope) |
| EvalFarm                   | Centralized evaluation platform for ML and NLP models.                       | [GitHub](https://github.com/evalfarm/evalfarm) |
| EMEval                     | Framework for model evaluation across multimodal tasks.                      | [GitHub](https://github.com/emeval/emeval) |
| EasyEval                   | Lightweight toolkit for evaluating deep learning and ML pipelines.            | [GitHub](https://github.com/easyeval/easyeval) |

## Model-Market – Evaluations (F)

| **Name**                  | **Known For**                                                                 | **Link** |
|----------------------------|-------------------------------------------------------------------------------|----------|
| FairEval                   | Evaluation framework for fairness and bias in ML models.                     | [GitHub](https://github.com/IBM/fairness-evaluation) |
| FastEval                   | Lightweight toolkit for fast evaluation of machine learning models.          | [GitHub](https://github.com/fast-eval/fast-eval) |
| FineEval                   | Fine-grained evaluation metrics for NLP and vision models.                   | [GitHub](https://github.com/fineeval/fineeval) |
| FlexEval                   | Modular evaluation suite supporting custom metrics and tasks.                 | [GitHub](https://github.com/flexeval/flexeval) |
| FoundryEval                | Model evaluation hub by Hugging Face for community benchmarks.               | [Website](https://huggingface.co/evaluate) |
| FLEET                      | Evaluation framework for federated learning models.                          | [GitHub](https://github.com/epfl-lts2/fleet) |
| FMEval                     | Amazon’s open-source toolkit for foundation model evaluation.                | [GitHub](https://github.com/awslabs/fmeval) |
| FullEval                   | End-to-end evaluation workflow for ML lifecycle management.                   | [GitHub](https://github.com/fulleval/fulleval) |

## Model-Market – Evaluations (G)

| **Name**                  | **Known For**                                                                 | **Link** |
|----------------------------|-------------------------------------------------------------------------------|----------|
| GLUE Benchmark             | Standard evaluation benchmark for NLP models.                                 | [Website](https://gluebenchmark.com/) |
| GloVeEval                  | Evaluation tools for word embeddings and vector representations.              | [GitHub](https://github.com/stanfordnlp/GloVe) |
| GaugeEval                  | Evaluation framework for generative AI model outputs.                         | [GitHub](https://github.com/gauge-eval/gauge) |
| GeoEval                    | Geographic and spatial ML model evaluation toolkit.                           | [GitHub](https://github.com/GeoEval/GeoEval) |
| GPT-Eval                   | Automated evaluation scripts for GPT-style language models.                   | [GitHub](https://github.com/EleutherAI/gpt-eval) |
| GradingML                  | ML evaluation toolkit for grading and scoring model predictions.              | [GitHub](https://github.com/gradingml/gradingml) |
| GreenML                     | Metrics and evaluation for energy-efficient ML models.                        | [GitHub](https://github.com/GreenML/GreenML) |
| GCP-Eval                   | Google Cloud Platform evaluation tools for ML pipelines.                      | [Website](https://cloud.google.com/ai-platform) |

## Model-Market – Evaluations (H)

| **Name**                  | **Known For**                                                                 | **Link** |
|----------------------------|-------------------------------------------------------------------------------|----------|
| HANS Benchmark             | Heuristic Analysis for NLI Systems, evaluating model robustness in NLP.       | [GitHub](https://github.com/facebookresearch/NLI-HANS) |
| HAR-Eval                   | Evaluation toolkit for Human Activity Recognition models.                     | [GitHub](https://github.com/har-eval/har-eval) |
| HealthML Benchmark         | Evaluation dataset and metrics for medical and health ML models.              | [Website](https://healthml.org/) |
| HuggingFace Eval           | Evaluation suite for various NLP and multimodal models hosted on HuggingFace. | [Website](https://huggingface.co/metrics) |
| HypeBench                  | Benchmarking platform for large-scale language model performance evaluation.  | [GitHub](https://github.com/hypebench/hypebench) |

