## Model-Market – Evaluations (A)

| **Name**                  | **Known For**                                                  | **Link** |
|----------------------------|----------------------------------------------------------------|----------|
| AutoEval                   | Automated evaluation toolkit for ML/DL models.                | [GitHub](https://github.com/microsoft/autoeval) |
| AI Explainability 360      | Toolkit by IBM for evaluating fairness and explainability.     | [GitHub](https://github.com/IBM/AIX360) |
| Arctic Evaluator           | Model evaluation pipeline for reproducible research.           | [GitHub](https://github.com/yourusername/arctic-evaluator) |
| Awesome Evaluation Tools   | Curated list of ML model evaluation frameworks.                | [GitHub](https://github.com/ml-tooling/awesome-evaluation-tools) |

## Model-Market – Evaluations (B)

| **Name**                  | **Known For**                                                  | **Link** |
|----------------------------|----------------------------------------------------------------|----------|
| BenchmarkML                | Framework for benchmarking ML models across tasks.             | [GitHub](https://github.com/benchmarkml/benchmarkml) |
| BigBench Evaluator          | Evaluation suite for BIG-bench NLP tasks.                     | [GitHub](https://github.com/allenai/BBH) |
| BlueEval                   | Evaluation framework for text-to-speech and audio models.      | [GitHub](https://github.com/blue-eval/blue-eval) |
| BLiMP Evaluations           | Benchmarking English linguistic minimal pairs in NLP models.  | [GitHub](https://github.com/alexwarstadt/blimp) |

